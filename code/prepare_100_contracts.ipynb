{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f78fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe705ced",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = '../Data/CUADv1.json'\n",
    "OUTPUT_CHUNKS_FILE = '../Data/processed_chunks.csv'\n",
    "OUTPUT_EMBEDDINGS_FILE = '../Data/embeddings.npy'\n",
    "NUM_CONTRACTS_TO_PROCESS = 100  # Subset for the 5-day sprint\n",
    "CHUNK_SIZE = 500  # Words\n",
    "OVERLAP = 100     # Words\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_text(text):\n",
    "    # normalize weird spaces but keep paragraph breaks\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    # collapse 3+ newlines → 2, and 2+ spaces → 1\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    text = re.sub(r'[ \\t]{2,}', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Splits text into overlapping chunks based on word count.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f818dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../Data/CUADv1.json...\n",
      "Processing 100 contracts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 870.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 2009 chunks from 100 contracts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Processing ---\n",
    "\n",
    "print(f\"Loading data from {INPUT_FILE}...\")\n",
    "with open(INPUT_FILE, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# List to hold all processed chunk data\n",
    "all_chunks_data = []\n",
    "\n",
    "print(f\"Processing {NUM_CONTRACTS_TO_PROCESS} contracts...\")\n",
    "for i, article in enumerate(tqdm(data['data'], total=NUM_CONTRACTS_TO_PROCESS)):\n",
    "    if i >= NUM_CONTRACTS_TO_PROCESS:\n",
    "        break\n",
    "        \n",
    "    contract_title = article['title']\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Iterate over ALL paragraphs in the article\n",
    "    for para_index, paragraph in enumerate(article['paragraphs']):\n",
    "        context = paragraph['context']\n",
    "        \n",
    "        # 1. Clean the text\n",
    "        cleaned_context = clean_text(context)\n",
    "        \n",
    "        # 2. Chunk the text\n",
    "        chunks = chunk_text(cleaned_context, CHUNK_SIZE, OVERLAP)\n",
    "        \n",
    "        # 3. Store chunks and metadata\n",
    "        for chunk_index, chunk in enumerate(chunks):\n",
    "            # Create a unique ID for every chunk\n",
    "            chunk_id = f\"{contract_title}_{para_index}_{chunk_index}\"\n",
    "            all_chunks_data.append({\n",
    "                'contract_title': contract_title,\n",
    "                'paragraph_index': para_index,\n",
    "                'chunk_id': chunk_id,\n",
    "                'chunk_text': chunk\n",
    "            })\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "print(f\"\\nCreated {len(all_chunks_data)} chunks from {NUM_CONTRACTS_TO_PROCESS} contracts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcad448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model (all-MiniLM-L6-v2)...\n",
      "Generating embeddings for 2009 chunks...\n",
      "Generating embeddings for 2009 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdee1ae840ed476f933a5d6accec598e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Embedding Generation ---\n",
    "\n",
    "print(\"Loading SBERT model (all-MiniLM-L6-v2)...\")\n",
    "# This will download the model the first time you run it\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract just the text to be embedded\n",
    "chunk_texts_to_embed = [d['chunk_text'] for d in all_chunks_data]\n",
    "\n",
    "print(f\"Generating embeddings for {len(chunk_texts_to_embed)} chunks...\")\n",
    "# This is the most time-consuming step\n",
    "# .encode() shows a progress bar by default\n",
    "all_embeddings = model.encode(chunk_texts_to_embed, show_progress_bar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c7a401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved processed chunks to ../Data/processed_chunks.csv\n",
      "Successfully saved embeddings to ../Data/embeddings.npy\n",
      "\n",
      "Data processing complete. You are ready to try FAISS again.\n"
     ]
    }
   ],
   "source": [
    "# --- Saving Output ---\n",
    "\n",
    "# 1. Save the processed chunks and metadata\n",
    "df_chunks = pd.DataFrame(all_chunks_data)\n",
    "df_chunks.to_csv(OUTPUT_CHUNKS_FILE, index=False)\n",
    "print(f\"\\nSuccessfully saved processed chunks to {OUTPUT_CHUNKS_FILE}\")\n",
    "\n",
    "# 2. Save the embeddings as a NumPy file\n",
    "np.save(OUTPUT_EMBEDDINGS_FILE, all_embeddings)\n",
    "print(f\"Successfully saved embeddings to {OUTPUT_EMBEDDINGS_FILE}\")\n",
    "print(\"\\nData processing complete. You are ready to try FAISS again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84614e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
