ğŸ“˜ RAG Backend Benchmarking (SQuAD Dataset)

This README provides setup and execution steps for benchmarking different vector databases (FAISS, Pinecone, and Azure AI Search) using the SQuAD v1.1 dataset.

All experiments can be run inside Jupyter or VSCode notebooks, so team members can easily reproduce results without command-line work.

â¸»

ğŸ§  Project Overview

Goal: Compare retrieval quality and performance across vector databases for a standard Question Answering (QA) task.

Databases:
	â€¢	FAISS â€“ Open-source, local, and CPU/GPU optimized.
	â€¢	Pinecone â€“ Cloud-native vector DB with scalable APIs.
	â€¢	Azure AI Search â€“ Hybrid semantic search engine combining keyword and vector search.

Dataset: SQuAD v1.1 (Stanford Question Answering Dataset)ï¿¼

Embedding Model: intfloat/e5-base-v2

â¸»

âš™ï¸ Environment Setup

Each teammate should:
	1.	Clone the project or sync with the shared repo.
	2.	Create and activate a virtual environment (e.g., myenv).
	3.	Install dependencies:

pip install sentence-transformers==3.* transformers==4.* faiss-cpu pandas numpy tqdm pinecone-client python-dotenv

	4.	Create a .env file in the project root:

PINECONE_API_KEY=your_pinecone_key_here


â¸»

ğŸ“‚ Project Structure

project/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ prepare_squad_faiss.py          # Builds FAISS index
â”‚   â”œâ”€â”€ faiis_eval.py                   # Evaluates FAISS retrieval
â”‚   â”œâ”€â”€ prepare_squad_pinecone.ipynb    # Uploads data & evaluates Pinecone
â”‚   â”œâ”€â”€ data_exploration.ipynb          # (Optional) dataset inspection
â”‚   â””â”€â”€ prepare_100_contracts.ipynb     # legacy CUAD setup
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ SQuAD/dev-v1.1.json             # Dataset file
â”‚   â””â”€â”€ squad_prepared/                 # Auto-generated embeddings & indices
â”‚
â””â”€â”€ README.md (this file)


â¸»

ğŸš€ Running FAISS Experiments (Notebook or VSCode)

Step 1. Prepare the Dataset

Run this cell in a notebook:

!python prepare_squad_faiss.py \
  --input ../Data/SQuAD/dev-v1.1.json \
  --out_dir ../Data/squad_prepared \
  --num_docs 0 \
  --max_tokens 384 \
  --stride 128 \
  --batch_size 96

This will create chunks, embeddings, and FAISS indices.

Step 2. Inspect Dataset

!python faiis_eval.py --inspect --squad ../Data/SQuAD/dev-v1.1.json

Step 3. Evaluate Retrieval

A) Semantic-match (lenient)

!python faiis_eval.py --eval --squad ../Data/SQuAD/dev-v1.1.json --k_list 1,3,5,10

B) String-only (stricter, fairer for comparison)

!python faiis_eval.py --eval --squad ../Data/SQuAD/dev-v1.1.json --k_list 1,3,5,10 --string_only

Optional: Global Search (no per-doc restriction)

!python faiis_eval.py --eval --squad ../Data/SQuAD/dev-v1.1.json --global_eval --k_list 1,3,5,10 --string_only


â¸»

â˜ï¸ Running Pinecone Experiments

Open and run prepare_squad_pinecone.ipynb.

The notebook steps include:
	1.	Loading the .env file (for PINECONE_API_KEY).
	2.	Initializing the Pinecone index.
	3.	Uploading SQuAD chunks and embeddings.
	4.	Running retrieval and computing Recall@K, EM, and F1.

ğŸ“¦ You can track progress with tqdm progress bars added to cells 10 and 11.

â¸»

ğŸ“Š Comparing Results

After both runs, record these metrics:

Metric	FAISS (Local)	Pinecone (Cloud)
Recall@1	0.999	0.812
Recall@3	1.000	0.873
Recall@5	1.000	0.884
Recall@10	1.000	0.891
MRR	0.999	â€“
EM	â€“	0.717
F1	â€“	0.769
N	10,570	1,000


â¸»

ğŸ’¬ Interpreting the Results
	â€¢	FAISS performs near-perfectly on local retrieval tasks (ideal baseline).
	â€¢	Pinecone reflects production-grade results with cloud API latency and real-world scaling.
	â€¢	Azure AI Search (optional) can later be tested with hybrid text + vector retrieval.

â¸»

ğŸ“ˆ Optional Extensions
	â€¢	Add latency and cost metrics (e.g., ms/query, $/month).
	â€¢	Include Azure AI Search comparison.
	â€¢	Extend to larger datasets (SQuAD train split).

â¸»

âœ… Team Reproducibility

Every teammate can open the notebooks directly in VSCode â†’ Jupyter Mode and execute sequentially.

Ensure the folder paths (../Data/...) remain consistent. All results (FAISS index, embeddings, and evaluation JSONs) will save automatically under ../Data/squad_prepared/.

â¸»

Authors: Team RAG Benchmark â€“ University of Louisville 2025